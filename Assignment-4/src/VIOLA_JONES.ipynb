{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VIOLA-JONES-FINAL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZvxVUELTtmV",
        "outputId": "8035c747-68bb-4a33-90e5-457bc2a64612"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML2nO4G9MeYJ"
      },
      "source": [
        "#### Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aK5Sz7QMeh0"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import timeit\n",
        "from sklearn.feature_selection import SelectPercentile, f_classif\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKrN60h9Mf42"
      },
      "source": [
        "#### Reading the training and testing dataset\n",
        "***I have used very less images due to not being able to upload the dataset on drive***<br>\n",
        "750 images for training and 50 images for testing for each class.\n",
        "\n",
        "For non-face images, scene recognition images are used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ayk1Rj1MgB8"
      },
      "source": [
        "def read_data():\n",
        "  train = []\n",
        "  test = []\n",
        "  train_size = 750\n",
        "  for i in range(750):\n",
        "    img = cv2.imread('/content/drive/My Drive/COMPUTER VISION/Faces/'+str(i)+'.jpg') \n",
        "    img = cv2.resize(img, (200, 200))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    #print(img.shape) \n",
        "    train.append([img,1])\n",
        "\n",
        "    img = cv2.imread('/content/drive/My Drive/COMPUTER VISION/non_faces/'+str(i+1)+'.jpg') \n",
        "    #print('/content/drive/My Drive/COMPUTER VISION/non_faces/'+str(i+1)+'.jpg',img)\n",
        "    img = cv2.resize(img, (200, 200))\n",
        "    #print(img.shape) \n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    #print(img.shape) \n",
        "    train.append([img,0])\n",
        "  \n",
        "  for i in range(750,800):\n",
        "    img = cv2.imread('/content/drive/My Drive/COMPUTER VISION/Faces/'+str(i)+'.jpg') \n",
        "    img = cv2.resize(img, (200, 200))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    #print(img.shape) \n",
        "    test.append([img,1])\n",
        "\n",
        "    img = cv2.imread('/content/drive/My Drive/COMPUTER VISION/non_faces/'+str(i)+'.jpg') \n",
        "    img = cv2.resize(img, (200, 200))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    #print(img.shape) \n",
        "    test.append([img,0])\n",
        "  \n",
        "  return train, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tqJaqANMhru"
      },
      "source": [
        "### Integral images\n",
        "Below function is used to compute integral image for a given input image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_CgOKxhMh-F"
      },
      "source": [
        "def get_integral_img(img):\n",
        "  integral_img = np.array(img, copy=True)  \n",
        "  integral_img = np.cumsum(integral_img,axis=0)\n",
        "  integral_img = np.cumsum(integral_img,axis=1)\n",
        "  return integral_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2IUHNRxMjxM"
      },
      "source": [
        "### This function is used to prepare input dataset by making integral image for each input image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEBuhds0MkAE"
      },
      "source": [
        "def prepare_data(data):\n",
        "  n = len(data)\n",
        "  X = []\n",
        "  y = []\n",
        "  #print(n)\n",
        "  for i in range(n):\n",
        "    X.append(get_integral_img(data[i][0]))\n",
        "    y.append(data[i][1])\n",
        "  X = np.array(X)\n",
        "  y = np.array(y)\n",
        "  return X,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5zQeQ-tMmPV"
      },
      "source": [
        "### Weights Initialization\n",
        "In this function, we initialize weights for each input image depending on its class and number of images in it's class\n",
        "\n",
        "The formula for calculating weights is : \n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1UOdfQ8gK_yUG5CH36OQiieyifgrmb3h1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK6P9c6qMmkN"
      },
      "source": [
        "def initialize_weights(labels):\n",
        "  n = labels.shape[0]\n",
        "  #print(n)\n",
        "  weights = np.zeros(n)\n",
        "  unique, counts = np.unique(labels, return_counts=True)\n",
        "  neg_cnt, pos_cnt = counts[0], counts[1]\n",
        "  #print(neg_cnt, pos_cnt)\n",
        "  for i in range(n):\n",
        "    if (labels[i]==0):\n",
        "      weights[i] = 0.5*(1/neg_cnt)\n",
        "    else:\n",
        "      weights[i] = 0.5*(1/pos_cnt)\n",
        "  return weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwBX2KZdMq-B"
      },
      "source": [
        "### Rectangle Feature class\n",
        "This class is used to store each feature easily as an object.\n",
        "There are 4 data members - \n",
        "- x : X coordinate of top left corner\n",
        "- y : Y coordinate of top left corner \n",
        "- W : Width of rectangle \n",
        "- H : Height of rectangle \n",
        "\n",
        "***calculate_feature_value()*** function is used to calculate sum of all pixels in given rectangle region using the fast method using integral images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YamttD9CMrSO"
      },
      "source": [
        "class Rectangle():\n",
        "  def __init__(self, row, col, width, height):\n",
        "    self.x = row\n",
        "    self.y = col\n",
        "    self.W = width\n",
        "    self.H = height\n",
        "\n",
        "  def calculate_feature_value(self, image):\n",
        "    #print(image.shape,[self.x+self.H],[self.y+self.W],\",,,\",[self.x],[self.y],\",,,\",[self.x],[self.y+self.W],\",,,\",[self.x+self.H],[self.y] )\n",
        "    return (image[self.x+self.H][self.y+self.W] + image[self.x][self.y]) - (image[self.x][self.y+self.W]+image[self.x+self.H][self.y])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGwMGnHMMylZ"
      },
      "source": [
        "### Feature 1 - 2 horizontal partitions\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1S6MThFxH2Hqwy9IoC83RS99GBqY1Eedt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyUgLZX5M0tO"
      },
      "source": [
        "def horizontal_rect_2(i, j, h, w, width, features):\n",
        "  top = Rectangle(i, j, w, h)\n",
        "  below = Rectangle(i+w, j, w, h)\n",
        "  if (i + 2 * w < width): \n",
        "    features.append(([below], [top]))\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuVxfmeiVUns"
      },
      "source": [
        "### Feature 2 - 3 Horizontal partitions\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1bV9PyTnYA_-m4vGrKUylKQQHCu6dTMX-)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9Tb0yqIM1AM"
      },
      "source": [
        "def horizontal_rect_3(i, j, h, w, width, features):\n",
        "  top = Rectangle(i, j, w, h)\n",
        "  mid = Rectangle(i+w, j, w, h)\n",
        "  bottom = Rectangle(i+2*w, j, w, h)\n",
        "  if (i + 3 * w < width): \n",
        "    features.append(([mid], [bottom, top]))\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99lGMp8mWH5J"
      },
      "source": [
        "### Feature 3 - 2 Vertical partitions\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=198bjYTg3UcWb1sWbp5DDTKK3mGFufHas)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbQtwoaRM1Lw"
      },
      "source": [
        "def vertical_rect_2(i, j, h, w, height, features):\n",
        "  left = Rectangle(i, j, w, h)\n",
        "  right = Rectangle(i, j+h, w, h)\n",
        "  if (j + 2 * h < height): \n",
        "    features.append(([left], [right]))\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JkevSQKWdCL"
      },
      "source": [
        "### Feature 4 - 3 Vertical partitions\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1ANEJYtV_V48ZhHCMDMHXda74QSEor4Qz)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n08wT9F3M1VW"
      },
      "source": [
        "def vertical_rect_3(i, j, h, w, height, features):\n",
        "  left = Rectangle(i, j, w, h)\n",
        "  mid = Rectangle(i, j+h, w, h)\n",
        "  right = Rectangle(i, j+2*h, w, h)\n",
        "  if (j + 3 * h < height): \n",
        "    features.append(([mid], [right, left]))\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMxTAlGvWw2J"
      },
      "source": [
        "### Feature 5 - 4 partitions\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1GGpt21XF_Kj7IclqyYyBWUmvfoKRBi2x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKi97O41M6uH"
      },
      "source": [
        "def rect_4(i, j, h, w, width, height, features):\n",
        "  top_L = Rectangle(i, j, w, h)\n",
        "  bot_L = Rectangle(i+w, j, w, h)\n",
        "  top_R = Rectangle(i, j+h, w, h)\n",
        "  bot_R = Rectangle(i+w, j+h, w, h)\n",
        "  if (i + 2 * w < width and j + 2 * h < height):\n",
        "    features.append(([bot_L, top_R], [top_L, bot_R]))\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98BmciW8W_Aa"
      },
      "source": [
        "#### Below function is used to prepare all features sets that have to be applied on all the images.\n",
        " It outputs a list of all features.\n",
        " Each entry has 2 item lists:\n",
        " - one contains rectangle regions that have to be added(WHITE REGION)\n",
        " - second is the list of all regions which have to be subtracted(BLACK REGIONS)\n",
        " \n",
        "All feature rectangle are of size 24 x 24.\n",
        "\n",
        "This region is equally divided based on the type of feature (we have 5 of them)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp-GQOGBMzGK"
      },
      "source": [
        "def prepare_rect_features():\n",
        "  width, height = 200, 200\n",
        "  window_size = 24\n",
        "  stride = 1\n",
        "  features = []\n",
        "  for i in range(0, width-window_size, stride):\n",
        "    for j in range(0, height-window_size, stride):\n",
        "      features = horizontal_rect_2(i, j, window_size//2, window_size, width, features)\n",
        "      features = horizontal_rect_3(i, j, window_size//3, window_size, width, features)\n",
        "      features = vertical_rect_2(i, j, window_size, window_size//2, height, features)\n",
        "      features = vertical_rect_3(i, j, window_size, window_size//3, height, features)\n",
        "      features = rect_4(i, j, window_size//2, window_size//2, width, height, features)\n",
        "\n",
        "  return features\n",
        "\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wZYfgq5MvVa"
      },
      "source": [
        "### Applying the features generated above on all traning images.\n",
        "On doing so, each image generates 129536 features.\n",
        "\n",
        "So, we get a data of dimention 1500 x 129536 for all 1500 input images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgx0KdqENBAa"
      },
      "source": [
        "def apply_features(data, features):\n",
        "  print(data.shape, len(features), type(data.shape[0]), data.shape[0])\n",
        "  X = np.zeros(( data.shape[0], len(features) ))\n",
        "  n = data.shape[0]\n",
        "  for i in range(n):\n",
        "    print(i)\n",
        "    j = 0\n",
        "    for pos_regions, neg_regions in features:\n",
        "      total_pos = sum([pos.calculate_feature_value(data[i]) for pos in pos_regions])\n",
        "      total_neg = sum([neg.calculate_feature_value(data[i]) for neg in neg_regions])\n",
        "      feature_val = total_pos - total_neg\n",
        "      X[i][j]= feature_val\n",
        "      j += 1\n",
        "\n",
        "  return X\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg_No6tiNBiT"
      },
      "source": [
        "### Preparing training data of integral images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5N7jpH_Mvqq",
        "outputId": "298db7e0-e077-4768-9299-d9f3de065c9a"
      },
      "source": [
        "train, test = read_data()\n",
        "X_train, y_train = prepare_data(train)\n",
        "X_test, y_test = prepare_data(test)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1500, 200, 200) (1500,)\n",
            "(100, 200, 200) (100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLipUnOuPxG1"
      },
      "source": [
        "### Making rectangular harr features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3qx6oCIPxO0",
        "outputId": "53d26b75-b8d9-43c6-8b95-3fcb9ee2572b"
      },
      "source": [
        "features = prepare_rect_features()\n",
        "print(len(features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "129536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PabI964KNhe8"
      },
      "source": [
        "#### Calculating value of each feature for all images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjpV0Hr_Nh-m",
        "outputId": "bf599f38-f4e4-41ef-8ff8-11e4dd03a7f7"
      },
      "source": [
        "\n",
        "start = timeit.default_timer()\n",
        "X = apply_features(X_train, features)\n",
        "stop = timeit.default_timer()\n",
        "print('Time: ', stop - start) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  3049.957049933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtr4n6oOfXkf",
        "outputId": "f2af7ed8-28ef-4fd5-81f3-32c5d004244d"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.6178e+04, -2.3191e+04, -1.2458e+04, ..., -6.2440e+03,\n",
              "        -7.0760e+03, -7.8990e+03],\n",
              "       [-1.3840e+03, -4.2739e+04, -7.2620e+03, ..., -1.5600e+02,\n",
              "         1.8000e+02,  6.0500e+02],\n",
              "       [ 3.2078e+04, -4.2780e+03,  2.2040e+03, ...,  1.8100e+02,\n",
              "         1.9900e+02,  2.1500e+02],\n",
              "       ...,\n",
              "       [ 1.8220e+03, -2.8570e+04,  1.2000e+01, ..., -1.5000e+01,\n",
              "        -1.9000e+01, -4.1000e+01],\n",
              "       [ 3.0330e+03, -1.1165e+04, -1.1950e+03, ..., -6.9400e+02,\n",
              "        -9.1300e+02, -9.3500e+02],\n",
              "       [-7.2700e+02, -4.1160e+04, -2.5010e+03, ..., -1.5500e+02,\n",
              "        -2.1200e+02, -2.6700e+02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK64MJmWNmuk"
      },
      "source": [
        "#### Computing feature every time afresh takes a lot of time\n",
        "So, I have saved the feature matrix computed in a csv file on my drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ1lshBVNnCD"
      },
      "source": [
        "np.savetxt(\"/content/drive/My Drive/COMPUTER VISION/face_features.csv\", X, delimiter=\",\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg4PhTffP_Qa"
      },
      "source": [
        "#### Reading the precomputed feature matrix to save time from computing it again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNJU1jczP_b7"
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/COMPUTER VISION/face_features.csv')  \n",
        "X_new = df.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4XhWY15Nte2"
      },
      "source": [
        "#### Feature Selection to get important feature to speed up training and for dimentionality reduction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWGYklRfNuku",
        "outputId": "49662716-a287-4c22-fcf9-4e167671bb66"
      },
      "source": [
        "indices = SelectPercentile(f_classif, percentile=10).fit(X, y_train).get_support(indices=True)\n",
        "X2 = X[:,indices]\n",
        "features2 = np.array(features)\n",
        "features2 = list(features2[indices])\n",
        "print(\"Selected %d potential features\" % len(features2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected 12954 potential features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoTZ92x_NwrU"
      },
      "source": [
        "#### Getting intial weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpZog48pNw-G",
        "outputId": "29e69bc2-395d-488f-a44c-127923b11b38"
      },
      "source": [
        "weights = initialize_weights(y_train)\n",
        "print(weights.shape)\n",
        "print(weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1500,)\n",
            "[0.00066667 0.00066667 0.00066667 ... 0.00066667 0.00066667 0.00066667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2IN9PCfNxqq"
      },
      "source": [
        "### AdaBoost algorithm for classifier learning\n",
        "\n",
        "For t = 1,...,T\n",
        " - Normalize the weights\n",
        " -  For each feature, j , train a classifier h_j which is restricted to using a single feature. The error is evaluated with respect to w_t,\n",
        "\n",
        "Error_j = ![picture](https://drive.google.com/uc?export=view&id=1lqVH-lL8YxDdbrcsoYTeaaNCpsq9saU3)\n",
        "\n",
        " - Choose the classifier, h_t , with the lowest error error_t.\n",
        " - Update the weights as follows :\n",
        "\n",
        " ![picture](https://drive.google.com/uc?export=view&id=1B7bUQW6j6YojcUYHUZaS4vBwqOr5okE0)\n",
        " \n",
        " where, \n",
        "  - e_i = 0 if example x_i is correctly classified\n",
        "  - e_i = 1 otherwise\n",
        "  - Beta_t = e_t / (1 - e_t)\n",
        "\n",
        "- The final strong classifier is:\n",
        "\n",
        " ![picture](https://drive.google.com/uc?export=view&id=1ZwU4JU_vADQa4b1ENDx2753qkQ-vL1tq)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ9h2cjJNx8d"
      },
      "source": [
        "def train(integral_imgs, X, y, weights, features, T):\n",
        "  alphas = []\n",
        "  clfs = []\n",
        "  for i in range(T):\n",
        "    #print(\"T = \",i)\n",
        "    norm_W = weights / np.linalg.norm(weights)\n",
        "    #print(\"training weak clfs...\")\n",
        "    weak_clf = train_weak(X.T, y, features, weights)\n",
        "    #print(\"selecting best weak clf...\")\n",
        "    clf, error, accuracy = select_best(weak_clf, weights, integral_imgs, y)\n",
        "    beta = error / (1.0 - error)\n",
        "    #print(\"calculating accuracy...\")\n",
        "    for i in range(len(accuracy)):\n",
        "      weights[i] = weights[i] * (beta ** (1 - accuracy[i]))\n",
        "    alpha = math.log(1.0/beta)\n",
        "    alphas.append(alpha)\n",
        "    clfs.append(clf)\n",
        "  return alphas, clfs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgsFfhjQN3if"
      },
      "source": [
        "#### Training a classifier for each of 129536 features. Each classifier takes into account only 1 feature for learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qg3kgxcN4Ap"
      },
      "source": [
        "\n",
        "def train_weak(X, y, features, weights):\n",
        "  idx_pos = np.where(y==1)\n",
        "  idx_neg = np.where(y==0)\n",
        "  tot_pos_wt = np.sum(weights[idx_pos])\n",
        "  tot_neg_wt = np.sum(weights[idx_neg])\n",
        "  CLFS = []\n",
        "  n = X.shape[0]\n",
        "  m = weights.shape[0]\n",
        "  for i in range(n):\n",
        "    pos_seen, neg_seen = 0, 0\n",
        "    pos_weights, neg_weights = 0, 0\n",
        "    min_error, best_feature, best_threshold, best_polarity = float('inf'), None, None, None\n",
        "    for j in range(m):\n",
        "      error = min(neg_weights + tot_pos_wt - pos_weights, pos_weights + tot_neg_wt - neg_weights)\n",
        "      if (error < min_error):\n",
        "        min_error = error\n",
        "        best_feature = features[i]\n",
        "        #print(best_feature)\n",
        "        best_threshold = X[i][j]\n",
        "        best_polarity = 1 if pos_seen > neg_seen else -1\n",
        "\n",
        "      if (y[j] == 1):\n",
        "        pos_seen += 1\n",
        "        pos_weights += weights[j]\n",
        "      else:\n",
        "        neg_seen += 1\n",
        "        neg_weights += weights[j]\n",
        "    clf = WeakClassifier(best_feature[0], best_feature[1], best_threshold, best_polarity)\n",
        "    CLFS.append(clf)\n",
        "  return CLFS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ4MIiPfhsUZ"
      },
      "source": [
        "#### Here we select the best classifier based on which one given leaast error upon classification of all images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4HD8bSGhstD"
      },
      "source": [
        "def select_best(classifiers, weights, X, y):\n",
        "  best_clf = None\n",
        "  best_error = np.inf\n",
        "  best_accuracy = None\n",
        "  n = X.shape[0]\n",
        "  for clf in classifiers:\n",
        "    error, accuracy = get_clf_stats(clf, X, y, weights)\n",
        "    if (error < best_error):\n",
        "      best_clf, best_error, best_accuracy = clf, error, accuracy\n",
        "  return best_clf, best_error, best_accuracy\n",
        "\n",
        "def get_clf_stats(clf, X, y, weights):\n",
        "  error, accuracy = 0, []\n",
        "  n = X.shape[0]\n",
        "  for i in range(n):\n",
        "    pred = abs(clf.classify(X[i]) - y[i])\n",
        "    accuracy.append(pred)\n",
        "    error += weights[i] * pred\n",
        "  error = error / X.shape[0]\n",
        "  return error, accuracy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFOAEkIGgHhK"
      },
      "source": [
        "#### Weak classifier\n",
        "To store a weak classifier easily, we have this class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZogFnwbngHve"
      },
      "source": [
        "class WeakClassifier:\n",
        "    def __init__(self, pos_regions, neg_regions, threshold, polarity):\n",
        "        self.pos_regions = pos_regions\n",
        "        self.neg_regions = neg_regions\n",
        "        self.threshold = threshold\n",
        "        self.polarity = polarity\n",
        "    \n",
        "    def classify(self, img):\n",
        "        total_pos = sum([pos.calculate_feature_value(img) for pos in self.pos_regions])\n",
        "        total_neg = sum([neg.calculate_feature_value(img) for neg in self.neg_regions])\n",
        "        feature_val = total_pos - total_neg\n",
        "        pred = self.polarity * feature_val\n",
        "        thresh = self.polarity * self.threshold\n",
        "        if (pred < thresh):\n",
        "          return 1\n",
        "        return 0\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8wdzrhWN4pG"
      },
      "source": [
        "#### Evaluation\n",
        "Below function is used to detect face in test dataset.\n",
        "\n",
        "We show confusion matrix and accuracy as output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyqMQWY9N5Mz"
      },
      "source": [
        "def predict(X, y, alphas, classifiers):\n",
        "  conf_mat = np.zeros((2,2))\n",
        "  alphas = np.array(alphas)\n",
        "  n = len(alphas)\n",
        "  threshold = np.sum(alphas)/2\n",
        "  num_imgs = X.shape[0]\n",
        "  for i in range(num_imgs):\n",
        "    tmp = np.zeros(n)\n",
        "    for j in range(n):\n",
        "      tmp[j] = classifiers[j].classify(X[i])\n",
        "    pred = np.matmul(alphas, tmp.T)\n",
        "    if ( pred >= threshold ):\n",
        "      out_class = 1\n",
        "    else:\n",
        "      out_class = 0\n",
        "    conf_mat[y[i]][out_class] += 1\n",
        "  #print(conf_mat)\n",
        "  return conf_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "guCkStsEN-6N"
      },
      "source": [
        "alphas, classifiers = train(X_train, X2, y_train, weights, features2, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9k7lK6JOB5v"
      },
      "source": [
        "conf_mat = predict(X_test, y_test, alphas, classifiers) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrdDPPF8nzPm",
        "outputId": "7911241e-3b78-4f95-d3ae-4b5c7bbcecbd"
      },
      "source": [
        "acc = (conf_mat[0][0] + conf_mat[1][1])/100 * 100\n",
        "print(\"Accuracy = \",acc)\n",
        "print(\"Confusion Matrix :\\n\",conf_mat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy =  57.99999999999999\n",
            "Confusion Matrix :\n",
            " [[30. 20.]\n",
            " [22. 28.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}